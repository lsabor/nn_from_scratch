
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/nn_model.ipynb

import numpy as np
import matplotlib.pyplot as plt
import math

class model():
    def __init__(self, X_dataset,Y_dataset, learning_rate = 0.1):
        self.X = X_dataset
        self.Y = Y_dataset
        self.W1 = np.random.rand(10, 784) -.5
        self.b1 = np.random.rand(10,1) -.5
        self.W2 = np.random.rand(10,10) -.5
        self.b2 = np.random.rand(10,1) -.5
        self.learning_rate = learning_rate

    def update(self,X_dataset, Y_dataset):
        self.X = X_dataset
        self.Y = Y_dataset

    def get_predictions(self,Y_hat):
        predictions = np.argmax(Y_hat,0)
        return predictions


    def accuracy(self, Y_hat, Y):

        predictions = self.get_predictions(Y_hat)
        return np.sum(predictions == Y) / Y.size

    def gradient_descent(self, iterations, learning_rate=0.5):

        m = self.X.shape[1]
        one_hot_Y = self.one_hot_encode(self.Y)
        for iteration in range(iterations):

            Z1, A1, Z2, Y_hat = self.forward_pass(X=self.X)

            DW1, Db1, DW2, Db2 = self.backwards_propagation(Y_hat,one_hot_Y, self.X,Z2, A1, Z1, m)
            self.update_params( DW1, Db1, DW2, Db2, self.learning_rate)

            accuracy = self.accuracy(Y_hat, self.Y)
            if iteration% 20 == 0:
                loss = self.cross_entropy_loss(one_hot_Y, Y_hat)
                print(f"Iteration={iteration} \n    {accuracy=}\n    {loss=}")


    def ReLU(self, n: float) -> float:
        """rectified linear unit activation function"""
        return np.maximum(n, 0)

    def ReLU_deriv(self, n: float) -> int:
        """"derivative of ReLU"""
        return n > 0

    def softmax(self, array: np.array) -> np.array:
        # collapses 1 dimension of array
        # np.exp(s) / sum(np.exp(s))
        return np.exp(array) / np.sum(np.exp(array), axis=0)

    def forward_pass(self, X):
        Z1 = np.dot(self.W1, X) + self.b1
        A1 = self.ReLU(Z1)
        Z2 = self.W2.dot(A1) + self.b2
        Y_hat = self.softmax(Z2)
        return Z1, A1, Z2, Y_hat

    def one_hot_encode(self, Y: np.array, classes = 10):
        # first instantiate 0's which should be an array of len(Y) max(Y)
        one_hot = np.zeros((Y.size,classes))
        one_hot[np.arange(Y.size), Y] = 1
        return one_hot.T

    def cross_entropy_loss(self, y_actual, y_predictions):
        num_samples=len(y_actual)
        class_targets =np.array(y_actual)
        y_actual = y_actual
        y_pred_clipped = np.clip(y_predictions, 1e-7, 1-1e-7)
        if len(y_actual.shape) ==1:
            targeted_predictions = y_pred_clipped[[range(num_samples), class_targets]]
        elif len(y_actual.shape) == 2:
            targeted_predictions = np.sum(y_pred_clipped.T * y_actual.T, axis=1)
        negative_log_likelihoods = -np.log(targeted_predictions)
        return np.mean(negative_log_likelihoods)


    def backwards_propagation(self, Y_hat, Y, X, Z2, A1, Z1, m):
        DZ2 =  -Y +Y_hat
        DW2 = np.dot(DZ2,A1.T) / m

        Db2 = np.sum(DZ2) / m

        DZ1 = np.dot(self.W2.T, DZ2) * self.ReLU_deriv(Z1)

        DW1 = np.dot(DZ1, self.X.T) / m
        Db1 =  np.sum(DZ1) / m
        return DW1, Db1, DW2, Db2


    def update_params(self, dW1, db1, dW2, db2, learning_rate):
        self.W1 = self.W1 - learning_rate*dW1
        self.b1 = self.b1 - learning_rate*db1
        self.W2 = self.W2 - learning_rate*dW2
        self.b2 = self.b2 - learning_rate*db2


    def make_prediction(self,X):
        print(X.shape)
        _,_,_, Y_hat = self.forward_pass(X)
        predictions = self.get_predictions(Y_hat)
        return predictions

    def test_predict(self,sample_index):
        current_image = self.X[:, sample_index, None]
        sample = self.Y[sample_index]
        current_image = current_image.reshape((28,28))*255

        plt.gray()
        plt.imshow(current_image, interpolation="nearest")
        plt.show()
        sample = self.Y[sample_index]
        X_sample = self.X[sample_index]
        prediction = self.make_prediction(self.X[:,sample_index, None])
        print(f"{sample=} , prediction = {prediction}. Correct = {sample == prediction[0]}")

    def export_weights(self):
        return self.W1, self.b1, self.W2, self.b2

    def import_weights(self, W1, b1, W2, b2):
        self.W1 = W1
        self.b1 = b1
        self.W2 = W2
        self.b2 = b2



