{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import our data\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# DATA FROM HERE: https://pjreddie.com/projects/mnist-in-csv/\n",
    "file_test = '../data/MNIST/mnist_test.csv'\n",
    "file_train = '../data/MNIST/mnist_train.csv'\n",
    "\n",
    "\n",
    "def get_data_from_csv(file: str) -> Tuple[np.array, int, int]:\n",
    "    \"\"\"takes data from file (csv type) and returns\n",
    "    a shuffled version of the data in an np.array form,\n",
    "    along with two ints:\n",
    "    m - number of test examples\n",
    "    n - number of points per example (including integrated labels)\"\"\"\n",
    "    assert os.path.exists(file), f\"{file} does not exist\"\n",
    "\n",
    "    data = pd.read_csv(file)\n",
    "    m, n = data.shape\n",
    "    data = np.array(data)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return (data, m, n)\n",
    "\n",
    "\n",
    "def get_labels_and_data_1st_column(data: np.array) -> Tuple[np.array, np.array]:\n",
    "    \"\"\"takes an np.array of data, returns (Transposed) labels (Y) and data (X)\"\"\"\n",
    "    data = data.T\n",
    "    Y = data[0]\n",
    "    X = data[1:]/255.\n",
    "    return (Y, X)\n",
    "\n",
    "\n",
    "data_test, m_test, n_test = get_data_from_csv(file_test)\n",
    "Y_test, X_test = get_labels_and_data_1st_column(data_test)\n",
    "\n",
    "data_train, m_train, n_train = get_data_from_csv(file_train)\n",
    "Y_train, X_train = get_labels_and_data_1st_column(data_train)\n",
    "\n",
    "assert n_test == n_train\n",
    "n = n_test\n",
    "m = m_test + m_train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"making sure that our Y_test/Y_train are actually labels\"\"\"\n",
    "\n",
    "assert Y_test.max() == 9\n",
    "assert Y_train.max() == 9\n",
    "assert X_test[0].max() != 9\n",
    "assert X_train[0].max() != 9\n",
    "\n",
    "# display(Y_test[:100])\n",
    "# display(Y_train[:100])\n",
    "# display(X_test[500][:100])\n",
    "# display(X_train[500][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FORWARD PASS\n",
    "Give X\n",
    "A0 = X :: [784,m]\n",
    "Z1[10,m] = W1[10,784] * X[784,m] + b1[10]\n",
    "A1[10,m] = RelU(Z1[10,m])\n",
    "Z2[10,m] = W2[10,10] * A1[10,m] + b2[10]\n",
    "Y_hat[10,m] = softmax(A2[10,m])\n",
    "Receive Y_hat\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def initialize_w_b():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10,1) - 0.5\n",
    "    W2 = np.random.rand(10,10) - 0.5\n",
    "    b1 = np.random.rand(10,1) - 0.5\n",
    "    return W1, b1, W2, b1\n",
    "\n",
    "def ReLU(Z: np.array) -> np.array:\n",
    "    \"\"\"rectified linear unit activation function\"\"\"\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_deriv(Z: np.array) -> np.array:\n",
    "    \"\"\"\"derivative of ReLU\"\"\"\n",
    "    return Z > 0\n",
    "\n",
    "def softmax(Z: np.array) -> np.array:\n",
    "    # collapses 1 dimension of array\n",
    "    eZ = np.exp(Z)\n",
    "    return eZ/sum(eZ)\n",
    "\n",
    "def loss(Y, Y_hat):\n",
    "    # TODO maybe add a catch for non-batched situations\n",
    "    Y_hat = Y_hat + 0.00001\n",
    "    return -np.einsum(\"ij,ij->j\",Y, np.log(Y_hat))\n",
    "\n",
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    Y_hat = softmax(Z2) \n",
    "    return Z1, A1, Z2, Y_hat\n",
    "\n",
    "def one_hot_encode(Y: np.array, classes = 10):\n",
    "    # first instantiate 0's which should be an array of len(Y) max(Y) \n",
    "    one_hot = np.zeros((Y.size,classes))\n",
    "    one_hot[np.arange(Y.size), Y] = 1\n",
    "    one_hot = one_hot.T\n",
    "    return one_hot\n",
    "\n",
    "# Y = one_hot_encode(Y)\n",
    "\n",
    "Y = Y_train\n",
    "X = X_train\n",
    "Z1, A1, Z2, Y_hat = forward_pass(X[:,1, None], W1, b1, W2, b2)\n",
    "print(np.sum(Y_hat,axis=1))\n",
    "\n",
    "    key:\n",
    "    VAR{a,b,...} | indicates that VAR has dimensions a x b x ...\n",
    "    VAR[i,j,...] | indicates the array or value at coordinates (i, j, ...) in VAR\n",
    "    \n",
    "    n = number of possible encodings # in general, n can change through a network,\n",
    "        but we're assuming that n is used for encodings and also layer size\n",
    "    m = number of inputs, batch size\n",
    "    x = number of datapoints per input\n",
    "\n",
    "(1)     X{x,m} = Input - m examples of x data points\n",
    "(2)     Y{n,m} = one-hot encoding of results: e.g. [[0,0,0,1,0,0], ... ] (with m encodings)\n",
    "(3)     W1{n,x} = weights applied to X\n",
    "(4)     b1{n,1} = biases applied to values going into Z1_n\n",
    "(5)     Z1{n,m} = pre-activation function values = W1{n,x} dot X{x,m} + b1{n,1}\n",
    "(6)     A1{n,m} = ReLU(Z1{n,m})\n",
    "(7)     W2{n,n} = weights applied to A1\n",
    "(8)     b2{n,1} = biases applied to values going into Z2_n\n",
    "(9)     Z2{n,m} = pre-softmax function values = W2{n,n} dot A1{n,m} + b2{n,1}\n",
    "(10)    Y_hat{n,m} = the estimate of Y = softmax(Z2{n,m})\n",
    "    \n",
    "    The forward pass looks like:\n",
    "    X * W1 + b1 = Z1\n",
    "    ReLU(Z1) = A1\n",
    "    A1 * W2 + b2 = Z2\n",
    "    softmax(Z2) = Y_hat\n",
    "\n",
    "    definition: Loss = L{m} = [L_0, L_1, ..., L_m] \n",
    "        = - sum over i of Y[i]*ln(Y_hat[i]) = -np.einsum(\"ij,ij->j\",Y, np.log(Y_hat))\n",
    "(11)    L{m} = -np.einsum(\"ij,ij->j\",Y, np.log(Y_hat))\n",
    "    def loss(Y, Y_hat):\n",
    "        return -np.einsum(\"ij,ij->j\",Y, np.log(Y_hat))\n",
    "    We set this loss specifically so that the derivative works out nicely\n",
    "\n",
    "\n",
    "    To minimize L, we want to see how L will change with respect to the variables\n",
    "    that we can control, namely W1, b1, W2, and b2.\n",
    "    \n",
    "    To calculate DW2 (dL/dW2), we use the chain rule:\n",
    "    DW2 = dL/dW2 = dL/dY_hat * dY_hat/dZ2 * dZ2/dW2\n",
    "    similarly for Db2:\n",
    "    Db2 = dL/db2 = dL/dY_hat * dY_hat/dZ2 * dZ2/db2\n",
    "    \n",
    "    But to calculate DW1 (dL/dW1), it is a little longer\n",
    "    DW1 = dL/dW1 = dL/dY_hat * dY_hat/dZ2 * dZ2/dA1 * dA1/dZ1 * dZ1/dW1\n",
    "    similarly for Db1:\n",
    "    Db1 = dL/db1 = dL/dY_hat * dY_hat/dZ2 * dZ2/dA1 * dA1/dZ1 * dZ1/db1\n",
    "\n",
    "    Now, let's start calculating each of these constituative derivatives.\n",
    "\n",
    "\n",
    "    dL/dY_hat:\n",
    "    dL/dY_hat.shape should be {n,m}\n",
    "    from (11)    L{m} = -np.dot(Y, np.log(Y_hat)):\n",
    "(12)    dL/dY_hat{n,m} = - {sum over i of} (Y[i] / Y_hat[i])\n",
    "    This shows us the opposite of exactly how Y_hat should change in order to minimize loss \n",
    "    across the n estimates for each of the m examples.\n",
    "\n",
    "\n",
    "    dY_hat/dZ2:\n",
    "    dY_hat/dZ2.shape should be {n,m}\n",
    "    from (10)    Y_hat{n,m} = the estimate of Y = softmax(Z2{n,m}):\n",
    "    given some i,j in range(n) and k,l in range(m):\n",
    "    Y_hat[i,k] changes with respect to Z2[j,l] only when k == l\n",
    "    for simplicity, assume k=l and thus drop those terms\n",
    "    dY_hat[i]/dZ2 has dimension {n}\n",
    "    dY_hat[i]/dZ2[j] = \n",
    "        if i == j --> softmax(Z2[j])*(1-softmax(Z2[j])\n",
    "        if i != j --> -softmax(Z2[i])*softmax(Z2[j])\n",
    "    dY_hat/dZ2 has dimension [n,n] for each entry in m\n",
    "    dY_hat/dZ2[i,j,k] =\n",
    "        if i == j --> softmax(Z2[j,k])*(1-softmax(Z2[j,k])\n",
    "        if i != j --> -softmax(Z2[i,k])*softmax(Z2[j,k])\n",
    "    for simplicity, call p[i, ...] = softmax(Z2[i, ...]). Thus:\n",
    "(13)    dY_hat/dZ2[i,j,k]{n,n,m} =\n",
    "            if i == j --> p[j,k]*(1-p[j,k])\n",
    "            if i != j --> -p[i,k]*p[j,k]\n",
    "\n",
    "\n",
    "    DZ2 = dL/dZ2:\n",
    "    DZ2.shape should be {n,m}\n",
    "    DZ2 = dL/dY_hat * dY_hat/dZ2\n",
    "    for now, drop m, so L has dim 1 while Z2 has dim {n}\n",
    "    let i,j in range(n)\n",
    "    from (13)   dY_hat/dZ2[i,j,k]{n,n,m} =\n",
    "                    if i == j --> p[j,k]*(1-p[j,k])\n",
    "                    if i != j --> -p[i,k]*p[j,k]:\n",
    "    dL/dZ2[j] = sum over i of dL/dY_hat[i] * dY_hat[i]/dZ2[j]\n",
    "        = {when i == j} - Y[j]/Y_hat[j] * Y_hat[j]*(1-Y_hat[j]) \n",
    "        + {sum over i when i != j of} (- (Y[i] / Y_hat[i]) * -Y_hat[i]*Y_hat[j] )\n",
    "        = -Y[j] * (1 - Y_hat[j]) - Y_hat[j] * {sum over i when i != j of} Y[i]\n",
    "        = -Y[j] + Y[j] * Y_hat[j] - Y_hat[j] * (-Y[j] + {sum over i of} Y[i]) # added Y[j] into summation\n",
    "        = -Y[j] + Y_hat[j] * (-Y[j] - (-Y[j] + 1)) # NOTE: {sum over i of} Y[i] = 1 since \n",
    "                                                   # Y[i] = 0 for all but 1 i, where it equals 1\n",
    "        = -Y[j] + Y_hat[j] * 1 = -Y[j] + Y_hat[j]\n",
    "    Adding back in k in range(m):\n",
    "    dL/dZ2[j,k] = -Y[j,k] + Y_hat[j,k]\n",
    "(14)    DZ2{n,m} = -Y + Y_hat\n",
    "\n",
    "\n",
    "    DW2 = dL/dW2:\n",
    "    DW2 = dL/dY_hat * dY_hat/dZ2 * dZ2/dW2 = DZ2 * dZ2/dw2\n",
    "    DW2.shape should be {n,n} (not m because W2 doesn't change across examples)\n",
    "    finding dZ2/dw2{n,n}:    \n",
    "    from (9)     Z2{n,m} = W2{n,n} dot A1{n,m} + b2{n,1}\n",
    "    let i,j,k in range(n), dropping m for now\n",
    "    Z2[i] = W2[i]{n} dot A1{n} + b2[i] = {sum over j} W2[i][j] * A1[j] + b2[i]\n",
    "    dZ2[i]/dW2[j,k]{1} = 0 if i != j, else A1[k]\n",
    "    dZ2[i]/dW2[i,k]{1} = A1[k]\n",
    "    dZ2/dW2{n} = A1\n",
    "    adding m back in: for l in range(m)\n",
    "    Z2[i,l]{1} = W2[i] dot A1[l] + b2[i]\n",
    "    dZ2[i,l]/dW2[i,k]{1} = A1[k,l]\n",
    "    dZ2[l]/dW2{n} = A1[l]{n}\n",
    "    dZ2/dW2{n,m} = A1{n,m}\n",
    "    This shows what you would multiply a delta_W with to get the difference in Z2\n",
    "    had you added that delta_W to W2 and recalulated Z2 that way\n",
    "\n",
    "    Dropping m again for a moment:\n",
    "    DW2{n,n} = DZ2 * dZ2/dW2 = DZ2{n} dot A1{n}\n",
    "    The derivative of the loss with respect to particular values of W2\n",
    "    To bring m back in the picture, we have to average over all of the losses accrued\n",
    "    during the training run. Namely m training examples:\n",
    "(15)    DW2{n,n} = 1/m * DZ2 * dZ2/dw2 = 1/m * DZ2{n,m} dot A1{n,m}.T{m,n}\n",
    "\n",
    "\n",
    "    Db2 = dL/db2:\n",
    "    Db2 = dL/dY_hat * dY_hat/dZ2 * dZ2/db2 = dZ2 * dZ2/db2\n",
    "    db2.shape should be {n} \n",
    "    finding dZ2/db2{n}:\n",
    "    from (9)     Z2{n,m} = W2{n,n} dot A1{n,m} + b2{n,1}\n",
    "    let i,j in range(n), dropping m for now\n",
    "    Z2[i] = W2[i]{n} dot A1{n} + b2[i] = {sum over j} W2[i][j] * A1[j] + b2[i]\n",
    "    dZ2[i]/db2[j]{1} = 0 if i != j, else 1\n",
    "    dZ2[i]/db2[i]{1} = 1\n",
    "    dZ2/db2{n} = 1\n",
    "    adding m back in: for l in range(m)\n",
    "    Z2[i,l]{1} = W2[i] dot A1[l] + b2[i]\n",
    "    dZ2[i,l]/db2[i]{1} = 1\n",
    "    dZ2[l]/db2{n} = 1{n}\n",
    "    dZ2/db2{n} = 1{n}\n",
    "\n",
    "    Dropping m again for a moment:\n",
    "    Db2{n} = DZ2 * dZ2/db2 = DZ2{n} * 1{n} = dZ2{n}\n",
    "    The derivative of the loss with respect to particular values of b2\n",
    "    To bring m back in the picture, we have to average over all of the losses accrued\n",
    "    during the training run. Namely m training examples:\n",
    "(16)    Db2{n} = 1/m * DZ2 * dZ2/dw2 = 1/m * 1{n} dot DZ2{n,m} = 1/m * np.sum(DZ2{n,m})\n",
    "\n",
    "\n",
    "    DA1 = dL/dA1:\n",
    "    DA1 = dL/dZ2 * dZ2/dA1\n",
    "    DA2.shape should be {n,m}\n",
    "    finding dZ2/dA1:\n",
    "    from (9)     Z2{n,m} = W2{n,n} dot A1{n,m} + b2{n,1}\n",
    "    let i,j in range(n), dropping m for now\n",
    "    Z2[i] = W2[i]{n} dot A1{n} + b2[i] = {sum over j} W2[i][j] * A1[j] + b2[i]\n",
    "    dZ2[i]/dA1[j] = W2[j,i]\n",
    "    dZ2/dA1[j] = W2[j]\n",
    "    dZ2/dA1 = W2\n",
    "    adding m back in: let k,l in range(m):\n",
    "    dZ2[:,k]/dA1[:,l] = 0 if l!= k, else W2\n",
    "    dZ2[:,k]/dA1[:,k] = W2\n",
    "    dZ2/dA1{n,n} = W2{n,n}\n",
    "\n",
    "    Dropping m again for a moment:\n",
    "    DA1{n} = DZ2 * dZ2/dA1 = DZ2{n} * W2{n,n} = W2.T{n,n} dot DZ2{n}\n",
    "    The derivative of loss with respect to a particular A1 value\n",
    "    Bringing m back in the picture is easy:\n",
    "{17}    DA1{n,m} = W2.T{n,n} dot DZ2{n,m}\n",
    "\n",
    "\n",
    "    DZ1 = DL/dZ1:\n",
    "    DZ1 = dL/dA1 * dA1/dZ1\n",
    "    DZ1.shape should be {n,m}\n",
    "    finding dA1/dZ1:\n",
    "    from (6)     A1{n,m} = ReLU(Z1{n,m}):\n",
    "    ReLU is applied item-wize on Z1, so the process is simple\n",
    "    dA1/dZ1{n,m} = ReLU_deriv(Z1{n,m})\n",
    "(18)    DZ1{n,m} = DA1 * dA1/dZ1 = DA1{n,m} * ReLU_deriv(Z1{n,m})\n",
    "    We are done already, and note \"*\" is multipliaction item-wize in this formula\n",
    "\n",
    "\n",
    "    DW1 & Db1:\n",
    "    process is identical to above. Thus:\n",
    "(19)    DW1{n,x} = 1/m * DZ2{n,m} dot X{x,m}.T{m,x}\n",
    "(20)    Db1{n} = 1/m * np.sum(DZ1{n,m})\n",
    "\n",
    "\n",
    "    Now we are done!\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    DZ2 = -Y + Y_hat\n",
    "    # (15) DW2{n,n} = 1/m * DZ2{n,m} dot A1{n,m}.T{m,n}\n",
    "    DW2 = 1/m * np.dot(DZ2,A1.T)\n",
    "    # (16) Db2{n} = 1/m * np.sum(DZ2{n,m})\n",
    "    Db2 = 1/m * np.sum(DZ2)\n",
    "\n",
    "    # {17}    DA1{n,m} = W2.T{n,n} dot DZ2{n,m}\n",
    "    DA1 = np.dot(W2.T,DZ2)\n",
    "    # (18)    DZ1{n,m} = DA1 * dA1/dZ1 = DA1{n,m} * ReLU_deriv(Z1{n,m})\n",
    "    DZ1 = DA1 * ReLU_deriv(Z1)\n",
    "\n",
    "    # (19)    DW1{n,x} = 1/m * DZ2{n,m} dot X{x,m}.T{m,x}\n",
    "    DW1 = 1/m * np.dot(DZ1, X.T)\n",
    "    # (20)    Db1{n} = 1/m * np.sum(DZ1{n,m})\n",
    "    Db1 = 1/m * np.sum(DZ1)\n",
    "\n",
    "    return DW1, Db1, DW2, Db2\n",
    "\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    \n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Accuracy: 0.18938648977482958\n",
      "\n",
      "Epoch 100\n",
      "Accuracy: 0.5634760579342989\n",
      "\n",
      "Epoch 200\n",
      "Accuracy: 0.7163952732545542\n",
      "\n",
      "Epoch 300\n",
      "Accuracy: 0.7698461641027351\n",
      "\n",
      "Epoch 400\n",
      "Accuracy: 0.8011633527225454\n",
      "\n",
      "Epoch 500\n",
      "Accuracy: 0.8228137135618927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "learning_rate = 0.1\n",
    "X = X_train\n",
    "Y_not_hot = Y_train\n",
    "\n",
    "Y = one_hot_encode(Y_not_hot)\n",
    "\n",
    "\n",
    "def train(X,Y,learning_rate,epochs):\n",
    "    m = X.shape[1]\n",
    "    W1, b1, W2, b2 = initialize_w_b()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        Z1, A1, Z2, Y_hat = forward_pass(X, W1, b1, W2, b2)\n",
    "        if (i+1) % (epochs//5) == 0 or not i:\n",
    "            print(f\"Epoch {i+1}\")\n",
    "            predictions = get_predictions(Y_hat)\n",
    "            print(f\"Accuracy: {get_accuracy(predictions, Y_not_hot)}\")\n",
    "            print()\n",
    "        dW1, db1, dW2, db2 = backwards_propagation(Y_hat, Y, Z2, A1, Z1, W1, b1, W2, b2, m, X)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "\n",
    "\n",
    "train(X,Y,.1, 500)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nn_from_scratch-5wfMGXWy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5740e5a6f631708b57c54ec6c0e0c4902b40036b869c78f2b20fbf3952ee788"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
